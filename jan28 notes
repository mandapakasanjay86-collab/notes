1. What is Big Data?

Deals with very large volumes of data (often > 1 TB, up to petabytes).
Requires high computation power.
Processed using parallel/distributed computation across clusters of machines.
Characterized by the 3Vs (Volume, Velocity, Variety) — sometimes extended to 5Vs (adding Veracity & Value).

2. Hadoop Ecosystem & Architecture
Core Components

HDFS (Hadoop Distributed File System) → Storage layer
MapReduce → Batch processing engine
YARN (Yet Another Resource Negotiator) → Resource management & job scheduling

HDFS (Storage)

Files split into blocks (default size: 128 MB or 256 MB in newer versions).
Blocks distributed across DataNodes → enables parallel read/write.
NameNode (Master): Stores metadata (file names, block locations).
DataNode (Workers): Store actual data blocks; handle read/write.
Replication for fault tolerance → Default replication factor = 3.
Trade-off: Replication & recovery take time for very large datasets.


MapReduce (Processing)

Map Phase: Processes input data in parallel (e.g., word count per record).
Reduce Phase: Aggregates/summarizes map outputs → final result.
Limitation: Every step reads from & writes to disk → slow due to heavy I/O.

3. Apache Spark Architecture
Why Spark over Hadoop MapReduce?

In-memory processing → avoids repeated disk I/O.
Ideal for iterative algorithms, real-time/streaming, and interactive analytics.
Up to 10–100x faster than MapReduce (especially for iterative/ML workloads).

Key Concepts

RDD (Resilient Distributed Dataset): Immutable, distributed, fault-tolerant collection; primarily in-memory.
In-memory caching of intermediate results → only final output written to disk.

Spark Components

Driver Program: Main control process; creates SparkContext; coordinates tasks.
Executors: Run on worker nodes; execute tasks; hold data partitions in memory/disk; return results.
Cluster Manager: Allocates resources (YARN, Kubernetes, Standalone, Mesos).

Nodes

Master → Manages cluster, scheduling, metadata.
Workers/Data Nodes → Store data, execute tasks, read/write.

================================================================================
4. HADOOP vs SPARK COMPARISON (2025/2026 Perspective)
================================================================================

┌─────────────────────┬──────────────────────┬─────────────────────────┐
│ Aspect              │ Hadoop (MapReduce)   │ Apache Spark            │
├─────────────────────┼──────────────────────┼─────────────────────────┤
│ Processing Model    │ Disk-based           │ In-memory (primary)     │
│                     │                      │ + disk fallback         │
├─────────────────────┼──────────────────────┼─────────────────────────┤
│ Speed               │ Slower               │ Much faster             │
│                     │ (disk I/O heavy)     │ (10–100× in many cases) │
├─────────────────────┼──────────────────────┼─────────────────────────┤
│ Disk I/O Pattern    │ Every step reads/    │ Minimal                 │
│                     │ writes disk          │ (only final output)     │
├─────────────────────┼──────────────────────┼─────────────────────────┤
│ Processing Types    │ Batch only           │ Batch + Streaming       │
│                     │                      │ + Interactive           │
├─────────────────────┼──────────────────────┼─────────────────────────┤
│ Ease of Use         │ Complex              │ Simpler APIs            │
│                     │ (Java-heavy)         │ (Scala, Python, Java, R)│
├─────────────────────┼──────────────────────┼─────────────────────────┤
│ Best Use Case Fit   │ Reliable large-scale │ ML, real-time,          │
│                     │ batch jobs           │ iterative, analytics    │
├─────────────────────┼──────────────────────┼─────────────────────────┤
│ Fault Tolerance     │ HDFS replication     │ RDD lineage +           │
│                     │                      │ replication             │
├─────────────────────┼──────────────────────┼─────────────────────────┤
│ Memory Usage        │ Minimal              │ Higher (in-memory)      │
│                     │                      │ but more efficient      │
└─────────────────────┴──────────────────────┴─────────────────────────┘

Summary & Recommendations:

Hadoop Remains Solid For:
  • Cost-effective, massive batch jobs
  • Scenarios where disk storage is preferable
  • Legacy systems with existing MapReduce investments
  • When processing constraints prevent large in-memory operations

Spark Dominates Modern Workloads:
  • Real-time data processing
  • AI/ML applications
  • Iterative algorithms
  • Interactive analytics
  • Streaming data
  • Data science & experimentation

General Trend:
  Spark has become the preferred choice for most modern data workloads,
  while Hadoop remains relevant for specific large-scale batch scenarios.







5. Ecosystem Tools on Hadoop

Hive – SQL-like interface (HiveQL) on HDFS.
Converts queries → MapReduce or Spark jobs.
Best for batch processing of structured data (logs, CSV, Parquet).

HBase – NoSQL database on HDFS.
Column-oriented; real-time random read/write.
Use cases: user profiles, counters, time-series, clickstream.


6. MySQL / SQL Built-in Functions (String & Others)
String Functions

Padding
LPAD(str, len, padstr) → left-pad
LPAD(title, 20, '*') → fills left side if < 20 chars

RPAD(str, len, padstr) → right-pad

Substring / Extraction
SUBSTRING(str, start, length) or SUBSTRING(str FROM start FOR len)
Index starts at 1 (unlike Python → 0).

LEFT(str, n) / RIGHT(str, n)
LOCATE(substr, str) → position of first match (1-based)

Concatenation & Manipulation
CONCAT(str1, str2, ...) or CONCAT_WS(sep, ...) (with separator)
REVERSE(str)
LENGTH(str) / CHAR_LENGTH(str) (characters vs bytes)
UPPER(str) / LOWER(str)
REPLACE(str, from_str, new_str)
TRIM([BOTH | LEADING | TRAILING] char FROM str) (also LTRIM, RTRIM)

Regular Expressions
REGEXP / RLIKE
title REGEXP '[eE]$' → ends with e or E
[^aeiouAEIOU]{3} → three consecutive non-vowels
^ = start, $ = end (no % wildcard like LIKE)



Math & Aggregation

SUM(), COUNT(), AVG(), MIN(), MAX()
FLOOR(), CEIL(), ROUND(), RAND(), POWER(), MOD()

Date / Time

NOW() → date + time
CURDATE() / CURRENT_DATE → date only
CURRENT_TIME → time only
Date arithmetic: NOW() - INTERVAL 1 DAY
DATE_FORMAT(), DATE_ADD(), DATE_SUB()

Type Casting

CAST(expr AS type) or CONVERT(expr, type)
Common: DECIMAL → INT, INT → VARCHAR, etc.

7. Data Concepts – Big Picture
Data → Information → Insights

Raw Data: Logs, clicks, transactions (no meaning alone)
Information: Cleaned, aggregated, processed
Insights: Actionable patterns, trends, decisions

Data Engineer vs Analyst vs Scientist vs ML Engineer

Data Engineer: Pipelines, ETL/ELT, reliability, scale
Data Analyst: Patterns, trends, BI, forecasting
Data Scientist: Stats + ML + domain → business solutions
ML/AI Engineer: Production models, recommendation, fraud, NLP

ETL vs ELT

ETL: Transform before load (traditional, on-prem)
ELT: Load raw → transform in cloud (cost-efficient, flexible)

Data Lake (Medallion Architecture)

Bronze → Raw ingestion
Silver → Cleaned, validated, business rules
Gold → Aggregated, reporting-ready

Data Warehouse

Fact Table: Measures (sales, revenue)
Dimension Table: Context (customer, time, product)
Schemas: Star / Snowflake

Change Data Capture (CDC)

Tracks only deltas → efficient ingestion

Orchestration

Manages dependencies, retries, failures (Airflow, Prefect, Dagster, etc.)
Scheduling = fixed time; Orchestration = smart dependency flow

8. SQL – Core Language Categories & Operations
Categories

DDL: CREATE, ALTER, DROP, TRUNCATE
DML: INSERT, UPDATE, DELETE
DQL: SELECT
DCL: GRANT, REVOKE
TCL: COMMIT, ROLLBACK, SAVEPOINT

Constraints

NOT NULL, UNIQUE, PRIMARY KEY, FOREIGN KEY, CHECK, DEFAULT
ON DELETE/UPDATE: RESTRICT, CASCADE, SET NULL

DELETE vs TRUNCATE vs DROP

DELETE → row-level, rollback possible, WHERE allowed
TRUNCATE → all rows, faster, no rollback, structure remains
DROP → entire table gone

Safe Mode (MySQL)

SET SQL_SAFE_UPDATES = 0; → allows mass UPDATE/DELETE
Works on indexed columns even when ON

Filtering & Grouping

WHERE → row-level (before GROUP BY)
HAVING → group-level (after aggregation)
LIKE → % (any chars), _ (single char)
IN / NOT IN, BETWEEN, IS NULL
ORDER BY … ASC/DESC
LIMIT / OFFSET

Execution Order (Logical)

FROM → JOIN
WHERE
GROUP BY
HAVING
SELECT
ORDER BY
LIMIT

